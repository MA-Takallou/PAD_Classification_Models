{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report, roc_curve, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.layers import LSTM, Dense\n",
    "#from tensorflow.keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.signal import butter, filtfilt\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os.path as path\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import scipy\n",
    "import os\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path.abspath(path.join(os.getcwd(),\"../\"))\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b356f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path + '\\\\new\\Healthy1.csv'\n",
    "d1 = pd.read_csv(filename, header=0)\n",
    "\n",
    "filename = path + '\\\\new\\Healthy2.csv'\n",
    "d2 = pd.read_csv(filename, header=0)\n",
    "\n",
    "filename = path + '\\\\new\\PAD1.csv'\n",
    "d3 = pd.read_csv(filename, header=0)\n",
    "\n",
    "filename = path + '\\\\new\\PAD2.csv'\n",
    "d4 = pd.read_csv(filename, header=0)\n",
    "\n",
    "filename = path + '\\\\new\\PAD3.csv'\n",
    "d5 = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1419af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.sort_index(inplace=True)\n",
    "d2.interpolate(method='spline', order=3, inplace=True)\n",
    "d2.ffill(inplace=True)  # fill NAs at the beginning of the column\n",
    "d2.bfill(inplace=True)  # fill NAs at the end of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7768eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d4.sort_index(inplace=True)\n",
    "d4.interpolate(method='spline', order=3, inplace=True)\n",
    "d4.ffill(inplace=True)  # fill NAs at the beginning of the column\n",
    "d4.bfill(inplace=True)  # fill NAs at the end of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c91241",
   "metadata": {},
   "outputs": [],
   "source": [
    "d5.sort_index(inplace=True)\n",
    "d5.interpolate(method='spline', order=3, inplace=True)\n",
    "d5.ffill(inplace=True)  # fill NAs at the beginning of the column\n",
    "d5.bfill(inplace=True)  # fill NAs at the end of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ed076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = d1.append(d2)\n",
    "df = df.append(d3)\n",
    "df = df.append(d4)\n",
    "df = df.append(d5)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb1e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc010fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'].value_counts().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc8ba5d",
   "metadata": {},
   "source": [
    "## Extracting acceleration from position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5488ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame()\n",
    "\n",
    "pos_cols = ['R.ASIS_x', 'R.ASIS_y', 'R.ASIS_z', 'Right_PSIS_x',\n",
    "       'Right_PSIS_y', 'Right_PSIS_z', 'L.ASIS_x', 'L.ASIS_y', 'L.ASIS_z',\n",
    "       'Left_PSIS_x', 'Left_PSIS_y', 'Left_PSIS_z', 'V.Sacral_x', 'V.Sacral_y',\n",
    "       'V.Sacral_z', 'Right_Troch_x', 'Right_Troch_y', 'Right_Troch_z',\n",
    "       'Right_mid_thigh_x', 'Right_mid_thigh_y', 'Right_mid_thigh_z',\n",
    "       'R.Thigh_x', 'R.Thigh_y', 'R.Thigh_z', 'R.Knee_x', 'R.Knee_y',\n",
    "       'R.Knee_z', 'R.Shank_x', 'R.Shank_y', 'R.Shank_z',\n",
    "       'Right_lower_shank_x', 'Right_lower_shank_y', 'Right_lower_shank_z',\n",
    "       'R.Ankle_x', 'R.Ankle_y', 'R.Ankle_z', 'R.Toe_x', 'R.Toe_y', 'R.Toe_z',\n",
    "       'R.Heel_x', 'R.Heel_y', 'R.Heel_z', 'Right_MTP_lat_x',\n",
    "       'Right_MTP_lat_y', 'Right_MTP_lat_z', 'Right_cal_lat_x',\n",
    "       'Right_cal_lat_y', 'Right_cal_lat_z', 'Left_Troch_x', 'Left_Troch_y',\n",
    "       'Left_Troch_z', 'Left_mid_thigh_x', 'Left_mid_thigh_y',\n",
    "       'Left_mid_thigh_z', 'L.Thigh_x', 'L.Thigh_y', 'L.Thigh_z', 'L.Knee_x',\n",
    "       'L.Knee_y', 'L.Knee_z', 'L.Shank_x', 'L.Shank_y', 'L.Shank_z',\n",
    "       'Left_lower_shank_x', 'Left_lower_shank_y', 'Left_lower_shank_z',\n",
    "       'L.Ankle_x', 'L.Ankle_y', 'L.Ankle_z', 'L.Toe_x', 'L.Toe_y', 'L.Toe_z',\n",
    "       'L.Heel_x', 'L.Heel_y', 'L.Heel_z', 'Left_MTP_lat_x', 'Left_MTP_lat_y',\n",
    "       'Left_MTP_lat_z', 'Left_cal_lat_x', 'Left_cal_lat_y', 'Left_cal_lat_z']\n",
    "\n",
    "\n",
    "for col in pos_cols:\n",
    "        pos = df[col].to_numpy()\n",
    "        time = df['Time'].to_numpy()\n",
    "        vel = np.gradient(pos, time)\n",
    "        acc = np.gradient(vel, time)\n",
    "        acc_df[f'acc_{col}'] = acc\n",
    "        \n",
    "acc_df['filename'] = df['filename'].values\n",
    "acc_df['PAD'] = df['PAD'].values\n",
    "acc_df['Time'] = df['Time'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000006aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f660c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = df[['Time', 'R.Heel_y']].iloc[0:200]\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(data_subset['Time'], data_subset['R.Heel_y'], label='R.Heel_y', linewidth=2, color='r')\n",
    "#plt.title('Time-series Plot of R.Heel_x')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = np.polyfit(time, signal, 1)\n",
    "polynomial = np.poly1d(coefficients)\n",
    "drift = polynomial(time)\n",
    "\n",
    "# Remove the drift\n",
    "corrected_signal = signal - drift\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Original Signal')\n",
    "plt.plot(time, signal)\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Fitted Drift')\n",
    "plt.plot(time, drift)\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Corrected Signal')\n",
    "plt.plot(time, corrected_signal)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cff0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Original and Corrected Signal\")\n",
    "plt.plot(time, signal, label='Original Signal', color='blue')\n",
    "plt.plot(time, corrected_signal, label='Corrected Signal', color='red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f202197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_df['Time'] = df['Time']\n",
    "data_subset = acc_df[['time', 'acc_R.Heel_y']].iloc[0:200]\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(data_subset['time'], data_subset['acc_R.Heel_y'], label='acc_R.Heel_y', linewidth=2, color='b')\n",
    "#plt.title('Time-series Plot of R.Heel_x')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49722d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df['Subject'] = acc_df['filename'].str.replace('p1c1_B1\\.trc$', '', regex=True).str.replace('p1c1_B1\\.csv$', '', regex=True).str.replace('c1p1_B1\\.csv$', '', regex=True).str.replace('c1p1_B1\\.trc$', '', regex=True).str.replace('p1c1_B01\\.csv$', '', regex=True).str.replace('p1c1_B11\\.csv$', '', regex=True)\n",
    "acc_df.Subject.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a36d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features\n",
    "features = ['acc_R.ASIS', 'acc_Right_PSIS', 'acc_L.ASIS', 'acc_Left_PSIS', 'acc_V.Sacral', 'acc_Right_Troch'\n",
    "       ,'acc_Right_mid_thigh','acc_R.Thigh', 'acc_R.Knee', 'acc_R.Shank', 'acc_Right_lower_shank', 'acc_R.Ankle'\n",
    "       , 'acc_R.Toe','acc_R.Heel', 'acc_Right_MTP_lat', 'acc_Right_cal_lat', 'acc_Left_Troch', 'acc_Left_mid_thigh'\n",
    "       , 'acc_L.Thigh', 'acc_L.Knee', 'acc_L.Shank', 'acc_Left_lower_shank', 'acc_L.Ankle', 'acc_L.Toe', 'acc_L.Heel'\n",
    "       , 'acc_Left_MTP_lat', 'acc_Left_cal_lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(features)\n",
    "n_cols = 2  # Number of subplots per row\n",
    "n_rows = int(np.ceil(n_features / n_cols))\n",
    "\n",
    "# Create a new figure and axis array\n",
    "fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(8 * n_cols, 6 * n_rows))\n",
    "\n",
    "# Flatten the axis array and remove extra subplots\n",
    "axs = axs.flatten()\n",
    "for ax in axs[n_features:]:\n",
    "    ax.remove()\n",
    "\n",
    "# Loop through each feature\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axs[idx]\n",
    "    \n",
    "    # Get the data for the current feature and slice the first 200 samples\n",
    "    feature_data = acc_df[[f\"{feature}_x\", f\"{feature}_y\", f\"{feature}_z\"]][:200]\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(feature_data)\n",
    "    normalized_data = pd.DataFrame(normalized_data, columns=feature_data.columns, index=feature_data.index)\n",
    "    \n",
    "    # Plot each dimension (x, y, z)\n",
    "    for dimension in ['x', 'y', 'z']:\n",
    "        ax.plot(normalized_data.index, normalized_data[f\"{feature}_{dimension}\"], label=dimension)\n",
    "    \n",
    "    # Set the title and labels\n",
    "    ax.set_title(f'{feature} - Dimensions (Normalized)')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Normalized Value')\n",
    "    \n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"acc.pdf\", format='pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['R.ASIS', 'Right_PSIS', 'L.ASIS', 'Left_PSIS', 'V.Sacral', 'Right_Troch'\n",
    "       ,'Right_mid_thigh','R.Thigh', 'R.Knee', 'R.Shank', 'Right_lower_shank', 'R.Ankle'\n",
    "       , 'R.Toe','R.Heel', 'Right_MTP_lat', 'Right_cal_lat', 'Left_Troch', 'Left_mid_thigh'\n",
    "       , 'L.Thigh', 'L.Knee', 'L.Shank', 'Left_lower_shank', 'L.Ankle', 'L.Toe', 'L.Heel'\n",
    "       , 'Left_MTP_lat', 'Left_cal_lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93821cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(features)\n",
    "n_cols = 2  # Number of subplots per row\n",
    "n_rows = int(np.ceil(n_features / n_cols))\n",
    "\n",
    "# Create a new figure and axis array\n",
    "fig, axs = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(8 * n_cols, 6 * n_rows))\n",
    "\n",
    "# Flatten the axis array and remove extra subplots\n",
    "axs = axs.flatten()\n",
    "for ax in axs[n_features:]:\n",
    "    ax.remove()\n",
    "\n",
    "# Loop through each feature\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axs[idx]\n",
    "    \n",
    "    # Get the data for the current feature and slice the first 200 samples\n",
    "    feature_data = df[[f\"{feature}_x\", f\"{feature}_y\", f\"{feature}_z\"]][:200]\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(feature_data)\n",
    "    normalized_data = pd.DataFrame(normalized_data, columns=feature_data.columns, index=feature_data.index)\n",
    "    \n",
    "    # Plot each dimension (x, y, z)\n",
    "    for dimension in ['x', 'y', 'z']:\n",
    "        ax.plot(normalized_data.index, normalized_data[f\"{feature}_{dimension}\"], label=dimension)\n",
    "    \n",
    "    # Set the title and labels\n",
    "    ax.set_title(f'{feature} - Dimensions (Normalized)')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Normalized Value')\n",
    "    \n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"motion.pdf\", format='pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df2 = acc_df[['Time', 'PAD', 'Subject']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492bf568",
   "metadata": {},
   "source": [
    "## Filtering acceleration using 4th order butterworth 20 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a27fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filter parameters\n",
    "cutoff_freq = 20 # Hz\n",
    "order = 4\n",
    "\n",
    "# Define the filter function\n",
    "def butterworth_filter(data, cutoff_freq, order, sampling_rate):\n",
    "    nyquist_freq = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_freq / nyquist_freq\n",
    "    b, a = butter(order, normal_cutoff, btype='lowpass', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "# Loop through each column (except the first two which are characters)\n",
    "for col in df.columns[1:-2]:\n",
    "    # Apply the Butterworth filter to the current column\n",
    "    filtered_data = butterworth_filter(df[col], cutoff_freq, order, 60)\n",
    "    # Replace the original data with the filtered data\n",
    "    df[col] = filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771efcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of features named 'features'\n",
    "features = ['acc_R.ASIS', 'acc_Right_PSIS', 'acc_Right_PSIS', 'acc_Right_PSIS', 'acc_L.ASIS', 'acc_L.ASIS'\n",
    "            , 'acc_V.Sacral', 'acc_V.Sacral', 'acc_V.Sacral', 'acc_L.ASIS', 'acc_Left_PSIS', 'acc_Left_PSIS', 'acc_Left_PSIS', 'acc_Right_Troch'\n",
    "            , 'acc_Right_Troch', 'acc_Right_Troch', 'acc_Right_mid_thigh', 'acc_Right_mid_thigh'\n",
    "            , 'acc_Right_mid_thigh', 'acc_R.Thigh', 'acc_R.Thigh', 'acc_R.Thigh', 'acc_R.Knee'\n",
    "            , 'acc_R.Knee', 'acc_R.Knee', 'acc_R.Shank', 'acc_R.Shank', 'acc_R.Shank', 'acc_Right_lower_shank'\n",
    "            , 'acc_Right_lower_shank', 'acc_Right_lower_shank', 'acc_R.Ankle', 'acc_R.Toe', 'acc_R.Toe'\n",
    "            , 'acc_R.Toe', 'acc_R.Heel', 'acc_R.Heel', 'acc_R.Heel', 'acc_Right_MTP_lat', 'acc_Right_cal_lat'\n",
    "            , 'acc_Left_Troch', 'acc_Left_Troch', 'acc_Left_Troch', 'acc_Left_mid_thigh', 'acc_L.Thigh'\n",
    "            , 'acc_L.Thigh', 'acc_L.Knee', 'acc_Left_lower_shank', 'acc_Left_lower_shank', 'acc_L.Ankle'\n",
    "            , 'acc_L.Ankle', 'acc_L.Toe', 'acc_L.Toe', 'acc_L.Toe', 'acc_L.Heel', 'acc_L.Heel'\n",
    "            , 'acc_Left_MTP_lat', 'acc_Left_MTP_lat', 'acc_Left_cal_lat', 'acc_Left_cal_lat']\n",
    "\n",
    "cutoff_freq = 15  # Hz\n",
    "order = 4\n",
    "sampling_rate = 60  # Assuming the sampling rate is 60 Hz, change it to the actual value\n",
    "\n",
    "\n",
    "#df.sort_index().interpolate(method='spline', order=3, inplace=True)\n",
    "    \n",
    "    \n",
    "filtered_data_list = []  # Create an empty list to store the filtered data\n",
    "\n",
    "for feature in features:\n",
    "    # Get the data for the current feature\n",
    "    feature_data = acc_df[[f\"{feature}_x\", f\"{feature}_y\", f\"{feature}_z\"]]\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(feature_data)\n",
    "    normalized_data = pd.DataFrame(normalized_data, columns=feature_data.columns, index=feature_data.index)\n",
    "\n",
    "    # Create a new DataFrame to store the filtered data for this feature\n",
    "    filtered_feature = pd.DataFrame()\n",
    "\n",
    "    for dimension in ['x', 'y', 'z']:\n",
    "        data = normalized_data[f\"{feature}_{dimension}\"]\n",
    "        filtered_data = butterworth_filter(data, cutoff_freq, order, sampling_rate)\n",
    "        filtered_feature[f\"{feature}_{dimension}\"] = filtered_data\n",
    "\n",
    "    # Store the filtered data for the current feature in the list\n",
    "    filtered_data_list.append(filtered_feature)\n",
    "\n",
    "# Combine the filtered data for all features into a single DataFrame\n",
    "filtered_df = pd.concat(filtered_data_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02efc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'acc_V.Sacral_x'\n",
    "\n",
    "# Calculate the correlation between the target column and all other columns\n",
    "correlations = filtered_df.corr()[target_column]\n",
    "\n",
    "# Print the correlations\n",
    "high_correlation_features = correlations[correlations > 0.8].index.tolist()\n",
    "\n",
    "# Print the list of features with > 0.8 correlation\n",
    "print(high_correlation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10123d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dr1 = ['R.ASIS_x', 'Right_PSIS_x', 'Left_PSIS_x', 'Right_Troch_x', 'Right_mid_thigh_x', 'R.Knee_x', 'R.Shank_x', 'Right_lower_shank_x', 'R.Ankle_x']\n",
    "dr2 = ['R.ASIS_y', 'Right_PSIS_y', 'L.ASIS_y', 'Left_PSIS_y', 'Right_Troch_y', 'Right_mid_thigh_y', 'R.Thigh_y', 'R.Knee_y', 'R.Knee_z', 'R.Shank_y', 'Right_lower_shank_y', 'R.Ankle_y', 'R.Toe_y', 'R.Heel_y', 'Right_MTP_lat_y', 'Right_cal_lat_y', 'Left_Troch_y', 'L.Shank_y', 'Left_MTP_lat_y', 'Left_cal_lat_y']\n",
    "dr3 = ['Right_PSIS_z', 'Left_PSIS_z']\n",
    "a = dr2 + dr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.drop(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4c8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = filtered_df.corr()\n",
    "\n",
    "# Function to get the first pair of highly correlated columns\n",
    "def get_first_highly_correlated_pair(corr_matrix, threshold=0.8):\n",
    "    # Upper triangle of correlations (to avoid duplicate pairs)\n",
    "    upper_triangle = np.triu(corr_matrix, k=1)\n",
    "    # Find row, col index for the first occurrence of correlation greater than threshold\n",
    "    row, col = np.where(np.abs(upper_triangle) > threshold)\n",
    "    \n",
    "    if row.size > 0:\n",
    "        # Return names of the first pair of highly correlated features\n",
    "        return corr_matrix.columns[row[0]], corr_matrix.columns[col[0]]\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "cols_to_drop = []\n",
    "\n",
    "while True:\n",
    "    # Get the first pair of highly correlated features\n",
    "    col1, col2 = get_first_highly_correlated_pair(correlation_matrix)\n",
    "    \n",
    "    # If no pair found, break out of loop\n",
    "    if not col1:\n",
    "        break\n",
    "\n",
    "    # Drop one of the columns from the dataframe and the correlation matrix\n",
    "    filtered_df.drop(col2, axis=1, inplace=True)\n",
    "    correlation_matrix.drop([col2], axis=1, inplace=True)\n",
    "    correlation_matrix.drop([col2], axis=0, inplace=True)\n",
    "\n",
    "    # Keep track of dropped columns\n",
    "    cols_to_drop.append(col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Generate a custom diverging colormap: Highly correlated in red, less correlated in blue\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "# Here, we consider correlations > 0.7 as high, you can adjust this threshold as needed\n",
    "sns.heatmap(correlation_matrix, cmap=cmap, vmax=.8, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)\n",
    "\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.savefig(\"correlation_heatmap.jpg\", format='jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c90535",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(filtered_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the filtering for the selected features without scaling to avoid data leakage\n",
    "# Define the filter function\n",
    "def butterworth_filter(data, cutoff_freq, order, sampling_rate):\n",
    "    nyquist_freq = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff_freq / nyquist_freq\n",
    "    b, a = butter(order, normal_cutoff, btype='lowpass', analog=False)\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "# Assuming you have a list of features named 'features'\n",
    "features = ['acc_R.ASIS_x', 'acc_Right_PSIS_x', 'acc_Right_PSIS_y', 'acc_Right_PSIS_z', 'acc_L.ASIS_x', 'acc_L.ASIS_y'\n",
    "            , 'acc_L.ASIS_z', 'acc_Left_PSIS_x', 'acc_Left_PSIS_y', 'acc_Left_PSIS_z', 'acc_Right_Troch_x'\n",
    "            , 'acc_Right_Troch_y', 'acc_Right_Troch_z', 'acc_Right_mid_thigh_x', 'acc_Right_mid_thigh_y'\n",
    "            , 'acc_Right_mid_thigh_z', 'acc_R.Thigh_x', 'acc_R.Thigh_y', 'acc_R.Thigh_z', 'acc_R.Knee_x'\n",
    "            , 'acc_R.Knee_y', 'acc_R.Knee_z', 'acc_R.Shank_x', 'acc_R.Shank_y', 'acc_R.Shank_z', 'acc_Right_lower_shank_x'\n",
    "            , 'acc_Right_lower_shank_y', 'acc_Right_lower_shank_z', 'acc_R.Ankle_y', 'acc_R.Toe_x', 'acc_R.Toe_y'\n",
    "            , 'acc_R.Toe_z', 'acc_R.Heel_x', 'acc_R.Heel_y', 'acc_R.Heel_z', 'acc_Right_MTP_lat_y', 'acc_Right_cal_lat_y'\n",
    "            , 'acc_Left_Troch_x', 'acc_Left_Troch_y', 'acc_Left_Troch_z', 'acc_Left_mid_thigh_x', 'acc_L.Thigh_y'\n",
    "            , 'acc_L.Thigh_z', 'acc_L.Knee_y', 'acc_Left_lower_shank_y', 'acc_Left_lower_shank_z', 'acc_L.Ankle_x'\n",
    "            , 'acc_L.Ankle_y', 'acc_L.Toe_x', 'acc_L.Toe_y', 'acc_L.Toe_z', 'acc_L.Heel_x', 'acc_L.Heel_z'\n",
    "            , 'acc_Left_MTP_lat_y', 'acc_Left_MTP_lat_z', 'acc_Left_cal_lat_y', 'acc_Left_cal_lat_z']\n",
    "\n",
    "cutoff_freq = 20  # Hz\n",
    "order = 4\n",
    "sampling_rate = 60  # Assuming the sampling rate is 60 Hz, change it to the actual value\n",
    "\n",
    "\n",
    "#df.sort_index().interpolate(method='spline', order=3, inplace=True)\n",
    "    \n",
    "    \n",
    "filtered_feature = pd.DataFrame()  # Create an empty list to store the filtered data\n",
    "\n",
    "for feature in features:\n",
    "    data = acc_df[feature]  # assuming acc_df is the original DataFrame containing your data\n",
    "    filtered_data = butterworth_filter(data, cutoff_freq, order, sampling_rate)\n",
    "    filtered_feature[feature] = filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.reset_index(drop=True)\n",
    "acc_df2 = acc_df2.reset_index(drop=True)\n",
    "filtered_df['PAD'] = acc_df2['PAD']\n",
    "filtered_df['Subject'] = acc_df2['Subject']\n",
    "filtered_df['Subject'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset subject based. 40 controls devided by the ratio of 60%, 20% and 20%. 73 PAD: 43 train, 15, 15.\n",
    "\n",
    "train = filtered_df[filtered_df['Subject'].isin(['s10', 's11', 's16', 's17', 's18', 's19', 's20', 's21', 's22',\n",
    "                                   's23', 's24', 's25', 's13', 's14', 's26', 's29', 's36 ', 's500',\n",
    "                                   's49', 's50', 's51', 's52', 's54', 's56',\n",
    "                                   's119', 's122', 's134', 's137', 's138', 's322', 's327', 's333', \n",
    "                                   's335', 's340', 's341', 's342', 's343', 's344', 's345', 's346', \n",
    "                                   's347', 's348', 's349', 's350', 's352','s136', 's361', 's362', \n",
    "                                   's364', 's366', 's368', 's370', 's371', 's372', 's432', 's433', \n",
    "                                   's436', 's440', 's441', 's444', 's445','s487', 's488', 's503', \n",
    "                                   's507', 's513', 's527'])]\n",
    "valid = filtered_df[filtered_df['Subject'].isin(['s31', 's33', 's34', 's35' , 's53', 's55', 's57', 's12',\n",
    "                                   's353', 's354', 's356', 's359', 's363', 's367', 's369', 's373',\n",
    "                                   's469', 's473', 's477', 's482', 's483', 's484', 's528'])]\n",
    "test = filtered_df[filtered_df['Subject'].isin(['s37', 's39', 's40', 's41', 's43', 's48', 's38', 's45',\n",
    "                                  's374', 's375', 's376', 's377', 's379', 's121', 's126', 's131',\n",
    "                                  's450', 's457', 's458', 's459', 's460', 's462', 's466'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdaad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some checks\n",
    "print(train['Subject'].nunique())\n",
    "print(valid['Subject'].nunique())\n",
    "print(test['Subject'].nunique())\n",
    "all_subjects = acc_df['Subject'].unique()\n",
    "assert set(train['Subject'].unique()) | set(valid['Subject'].unique()) | set(test['Subject'].unique()) == set(all_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data on the subjects\n",
    "\n",
    "# Create a list of unique subject IDs\n",
    "unique_subject_ids = train['Subject'].unique()\n",
    "\n",
    "# Shuffle the order of the subject IDs\n",
    "shuffled_subject_ids = shuffle(unique_subject_ids, random_state=42)\n",
    "\n",
    "# Create a new DataFrame for the shuffled data\n",
    "shuffled_tr = pd.DataFrame(columns=train.columns)\n",
    "\n",
    "# Iterate over the shuffled subject IDs and append the corresponding data to the new DataFrame\n",
    "for subject_id in shuffled_subject_ids:\n",
    "    subject_data = train[train['Subject'] == subject_id]\n",
    "    shuffled_tr = shuffled_tr.append(subject_data)\n",
    "\n",
    "# Reset the index of the shuffled DataFrame\n",
    "shuffled_tr.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25681f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_subject_ids = valid['Subject'].unique()\n",
    "\n",
    "shuffled_subject_ids = shuffle(unique_subject_ids, random_state=42)\n",
    "\n",
    "shuffled_va = pd.DataFrame(columns=valid.columns)\n",
    "\n",
    "for subject_id in shuffled_subject_ids:\n",
    "    subject_data = valid[valid['Subject'] == subject_id]\n",
    "    shuffled_va = shuffled_va.append(subject_data)\n",
    "\n",
    "shuffled_va.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_subject_ids = test['Subject'].unique()\n",
    "\n",
    "shuffled_subject_ids = shuffle(unique_subject_ids, random_state=42)\n",
    "\n",
    "shuffled_te = pd.DataFrame(columns=test.columns)\n",
    "\n",
    "for subject_id in shuffled_subject_ids:\n",
    "    subject_data = test[test['Subject'] == subject_id]\n",
    "    shuffled_te = shuffled_te.append(subject_data)\n",
    "\n",
    "shuffled_te.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c1e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tain_sub = shuffled_tr['Subject']\n",
    "val_sub = shuffled_va['Subject']\n",
    "test_sub = shuffled_te['Subject']\n",
    "shuffled_tr.drop(['Subject'], axis = 1, inplace = True)\n",
    "shuffled_va.drop(['Subject'], axis = 1, inplace = True)\n",
    "shuffled_te.drop(['Subject'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['acc_R.ASIS_x', 'acc_Right_PSIS_x', 'acc_Right_PSIS_y', 'acc_Right_PSIS_z', 'acc_L.ASIS_x', 'acc_L.ASIS_y'\n",
    "        , 'acc_L.ASIS_z', 'acc_Left_PSIS_x', 'acc_Left_PSIS_y', 'acc_Left_PSIS_z', 'acc_Right_Troch_x', 'acc_Right_Troch_y'\n",
    "        , 'acc_Right_Troch_z', 'acc_Right_mid_thigh_x', 'acc_Right_mid_thigh_y', 'acc_Right_mid_thigh_z', 'acc_R.Thigh_x'\n",
    "        , 'acc_R.Thigh_y', 'acc_R.Thigh_z', 'acc_R.Knee_x', 'acc_R.Knee_y', 'acc_R.Knee_z', 'acc_R.Shank_x', 'acc_R.Shank_y'\n",
    "        , 'acc_R.Shank_z', 'acc_Right_lower_shank_x', 'acc_Right_lower_shank_y', 'acc_Right_lower_shank_z', 'acc_R.Ankle_y'\n",
    "        , 'acc_R.Toe_x', 'acc_R.Toe_y', 'acc_R.Toe_z', 'acc_R.Heel_x', 'acc_R.Heel_y', 'acc_R.Heel_z', 'acc_Right_MTP_lat_y'\n",
    "        , 'acc_Right_cal_lat_y', 'acc_Left_Troch_x', 'acc_Left_Troch_y', 'acc_Left_Troch_z', 'acc_Left_mid_thigh_x'\n",
    "        , 'acc_L.Thigh_y', 'acc_L.Thigh_z', 'acc_L.Knee_y', 'acc_Left_lower_shank_y', 'acc_Left_lower_shank_z'\n",
    "        , 'acc_L.Ankle_x', 'acc_L.Ankle_y', 'acc_L.Toe_x', 'acc_L.Toe_y', 'acc_L.Toe_z', 'acc_L.Heel_x', 'acc_L.Heel_z'\n",
    "        , 'acc_Left_MTP_lat_y', 'acc_Left_MTP_lat_z', 'acc_Left_cal_lat_y', 'acc_Left_cal_lat_z']\n",
    "\n",
    "Xa = shuffled_tr[cols]\n",
    "ya = shuffled_tr['PAD']\n",
    "\n",
    "Xv = shuffled_va[cols]\n",
    "yv = shuffled_va['PAD']\n",
    "\n",
    "Xt = shuffled_te[cols]\n",
    "yt = shuffled_te['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c16a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in Xa.select_dtypes(include=['float64']).columns:\n",
    "    Xa[cols] = Xa[cols].astype('float32')\n",
    "    \n",
    "for cols in Xv.select_dtypes(include=['float64']).columns:\n",
    "    Xv[cols] = Xv[cols].astype('float32')\n",
    "    \n",
    "for cols in Xt.select_dtypes(include=['float64']).columns:\n",
    "    Xt[cols] = Xt[cols].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af400012",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_finite = Xa.all().all()\n",
    "print(\"All values are finite:\", all_finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bcc149",
   "metadata": {},
   "outputs": [],
   "source": [
    "ya = le.fit_transform(ya)\n",
    "yv = le.transform(yv)\n",
    "yt = le.transform(yt)\n",
    "\n",
    "Xa = scaler.fit_transform(Xa)\n",
    "Xv = scaler.transform(Xv)\n",
    "Xt = scaler.transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xa.shape)\n",
    "print(Xv.shape)\n",
    "print(Xt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "def lstm_data_transform(x_data, y_data, num_steps= num_steps):\n",
    "    \"\"\" Changes data to the format for LSTM training \n",
    "for sliding window approach \"\"\"\n",
    "    # Prepare the list for the transformed data\n",
    "    X, y = list(), list()\n",
    "    # Loop of the entire data set\n",
    "    for i in range(x_data.shape[0]):\n",
    "        # compute a new (sliding window) index\n",
    "        end_ix = i + num_steps\n",
    "        # if index is larger than the size of the dataset, we stop\n",
    "        if end_ix >= x_data.shape[0]:\n",
    "            break\n",
    "        # Get a sequence of data for x\n",
    "        seq_X = x_data[i:end_ix]\n",
    "        # Get only the last element of the sequency for y\n",
    "        seq_y = y_data[end_ix]\n",
    "        # Append the list with sequencies\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "    # Make final arrays\n",
    "    x_array = np.array(X)\n",
    "    y_array = np.array(y)\n",
    "    return x_array, y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ebddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = np.array(Xa)\n",
    "Xv = np.array(Xv)\n",
    "Xt = np.array(Xt)\n",
    "\n",
    "ya = np.array(ya)\n",
    "yv = np.array(yv)\n",
    "yt = np.array(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "# training set\n",
    "(Xa, ya) = lstm_data_transform(Xa, ya, num_steps=num_steps)\n",
    "assert Xa.shape[0] == ya.shape[0]\n",
    "# validation set\n",
    "(Xv, yv) = lstm_data_transform(Xv, yv, num_steps=num_steps)\n",
    "assert Xv.shape[0] == yv.shape[0]\n",
    "# test set\n",
    "(Xt, yt) = lstm_data_transform(Xt, yt, num_steps=num_steps)\n",
    "assert Xt.shape[0] == yt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b9ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "# Assuming Xa, Xv, Xt, ya, yv, yt are already defined elsewhere\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Define hyperparameters to be tuned\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-3, log=True)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    lstm_units = trial.suggest_int(\"lstm_units\", 2, 128, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"rmsprop\"])\n",
    "\n",
    "    # Model definition\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, activation='tanh', input_shape=(100, 57), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Choose optimizer\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(Xa, ya, validation_data=(Xv, yv), epochs=1, batch_size=batch_size, callbacks=[es_cb], verbose=1)\n",
    "\n",
    "    # Return the best validation accuracy\n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    return val_acc\n",
    "\n",
    "print('training is started!')\n",
    "print(datetime.datetime.now())\n",
    "# Start the Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "\n",
    "# Print out the results\n",
    "print(datetime.datetime.now())\n",
    "print(str(study))\n",
    "\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best trial: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831808fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def earlystopping(min_delta, patience):\n",
    "    es_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             min_delta=min_delta,\n",
    "                                             patience=patience,\n",
    "                                             restore_best_weights=True)\n",
    "    return es_cb\n",
    "\n",
    "\n",
    "num_steps = 100\n",
    "num_features = 57\n",
    "max_epochs = 1\n",
    "patience = 5\n",
    "min_delta = 1e-5\n",
    "batch_size = 32\n",
    "Beta1 = 0.9\n",
    "Beta2 = 0.999\n",
    "Epsilon = 10^-8\n",
    "optimizer = 'adam'\n",
    "best_lr = 0.001\n",
    "\n",
    "model_16 = Sequential()\n",
    "#model_2.add(LSTM(64, activation='relu', input_shape=(num_steps, num_features), return_sequences=True))\n",
    "#model_2.add(Dropout(0.2))\n",
    "#model_2.add(LSTM(units = 32, activation='relu', return_sequences = True))\n",
    "#model_2.add(Dropout(0.5))\n",
    "#model_2.add(LSTM(units = 16, activation='relu', return_sequences = True))\n",
    "#model_2.add(Dropout(0.5))\n",
    "model_16.add(LSTM(units = 4, activation='tanh', dropout=0.5, recurrent_dropout=0.5, return_sequences = True))\n",
    "model_16.add(Dropout(0.7))\n",
    "model_16.add(LSTM(units = 4, activation='tanh', dropout=0.5, recurrent_dropout=0.5, return_sequences = True))\n",
    "model_16.add(Dropout(0.5))\n",
    "model_16.add(LSTM(units = 4, activation='tanh', dropout=0.5, recurrent_dropout=0.5, return_sequences = True))\n",
    "model_16.add(Dropout(0.5))\n",
    "model_16.add(LSTM(units = 4, activation='tanh', dropout=0.5, recurrent_dropout=0.5, return_sequences = False))\n",
    "model_16.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "if optimizer == \"adam\":\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=best_lr)\n",
    "else:  # It's redundant here since we know the best optimizer is Adam, but it's good to have for completeness\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=best_lr)\n",
    "    \n",
    "\n",
    "# compile\n",
    "model_16.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "# train\n",
    "model_16_history = model_16.fit(Xa, ya,\n",
    "                        epochs=max_epochs,\n",
    "                        validation_data=(Xv, yv),\n",
    "                        shuffle=False,\n",
    "                        callbacks=[earlystopping(min_delta, patience)],\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4673ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model_16.evaluate(Xt, yt, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97363140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "test_predict = model_16.predict(Xt)\n",
    "y_pred = (test_predict > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "import seaborn as sn\n",
    "cm = confusion_matrix(yt, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(yt, y_pred)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, range(2), range(2))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.title(\"Accuracy: %.2f%%\" % (accuracy_score(yt, y_pred)*100))\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import repeat\n",
    "final = pd.DataFrame(np.concatenate((y_pred.reshape(len(y_pred),1), yt.reshape(len(yt),1)),1))\n",
    "final.columns = ['Predict','Actual']\n",
    "\n",
    "final = final.reset_index()\n",
    "\n",
    "sub = pd.DataFrame(test_sub[100:]).reset_index(drop=True)\n",
    "\n",
    "final1 = pd.concat([final, sub], axis=1)\n",
    "\n",
    "final1['pred'] = final1.groupby('Subject')['Predict'].transform('mean')\n",
    "final1['prediction_result'] = (final1.pred > 0.5)\n",
    "final1['Prediction'] = final1['prediction_result'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = final1[['Subject', 'Prediction', 'Actual']]\n",
    "final1 = final1.groupby(['Subject'], as_index = False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351599cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt1 = final1['Actual']\n",
    "yp = final1['Prediction']\n",
    "cm1 = confusion_matrix(yt1, yp)\n",
    "print(cm1)\n",
    "accuracy_score(yt1, yp)\n",
    "\n",
    "df_cm1 = pd.DataFrame(cm1, range(2), range(2))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm1, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.title(\"Accuracy: %.2f%%\" % (accuracy_score(yt1, yp)*100))\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
